# Emotional-Analysis-in-Chat-Conversations-and-Classification

### code link -> https://www.kaggle.com/code/sparshrastogicsv/emotion-analysis-and-prediction-using-nlp-lstm-93/
## -> Emotion Analysis in Chat Conversations using LSTM

### -> Project Overview

This project focuses on detecting human emotions from chat-based text conversations using deep learning. The model classifies user messages into different emotional categories (such as *happy, sad, angry, neutral, fear,* etc.) by analyzing textual cues.
Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) layers are used to capture the sequential dependencies in language and context transitions within conversations.

Achieved **93% classification accuracy**, making it highly effective for emotion-aware systems such as:

* Intelligent chatbots
* Customer support systems
* Mental health monitoring applications
* Sentiment-based recommendation engines

---

### ðŸ“š Dataset Information

* **Source:** Chat-based emotion dataset (e.g., DailyDialog or custom annotated chat corpus).
* **Structure:** Each record contains a message text and its corresponding emotion label.
* **Emotion Categories:** Typically includes `happy`, `sad`, `angry`, `surprise`, `fear`, `disgust`, `neutral`.
* **Preprocessing Steps:**

  * Lowercasing and punctuation removal
  * Tokenization using `Tokenizer` from Keras
  * Padding sequences for uniform input length
  * Train-test split (e.g., 80â€“20)

---

### ->Model Architecture

The model uses a **Sequential LSTM** architecture implemented in TensorFlow/Keras:

```
Embedding Layer â†’ LSTM Layer â†’ Dropout â†’ Dense Layer (Softmax)
```

**Details:**

* **Embedding Dimension:** 100
* **LSTM Units:** 128
* **Dropout:** 0.3 to prevent overfitting
* **Dense Output Layer:** Softmax activation for multi-class classification
* **Loss Function:** Categorical Crossentropy
* **Optimizer:** Adam (learning rate tuned)
* **Evaluation Metric:** Accuracy, F1-score

---

### -> How to Run the Project

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/emotion-analysis-lstm.git
   cd emotion-analysis-lstm
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Run the notebook**

   ```bash
   jupyter notebook emotion-analysis-and-prediction-using-lstm-93.ipynb
   ```

4. **(Optional)** Export trained model

   ```python
   model.save("emotion_lstm_model.h5")
   ```

---

### -> Results

| Metric    | Value   |
| --------- | ------- |
| Accuracy  | **93%** |
| Precision | ~0.92   |
| Recall    | ~0.91   |
| F1-score  | ~0.92   |

**Confusion matrix** and **training/validation loss curves** show strong performance consistency with minimal overfitting.

---

### -> Future Work

* Incorporate **attention mechanisms** or **BiLSTM** for improved contextual understanding.
* Compare performance with **transformer-based models (BERT, RoBERTa)**.
* Expand dataset to include **multi-turn dialogue context**.
* Deploy model as a **Flask/FastAPI service** or integrate with chat applications.

---

### -> Tech Stack

* **Python 3.x**
* **TensorFlow / Keras**
* **NumPy, Pandas, Matplotlib, Seaborn**
* **Scikit-learn**

---

### -> Author

**Sparsh Rastogi**
AI Branch, IIT Patna
Adobe Product Intern | Research Consultant (WorldQuant)

---
